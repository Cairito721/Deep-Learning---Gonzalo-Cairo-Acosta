{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "<table align=\"center\">\n",
    " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/CNN/Classical_Architectures_CNN.ipynb\">\n",
    "        <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/CNN/Classical_Architectures_CNN.ipynb\">\n",
    "        <img src=\"https://i.ibb.co/xfJbPmL/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classic network architectures:**\n",
    "\n",
    "- LeNet-5\n",
    "- AlexNet\n",
    "- VGG 16\n",
    "\n",
    "**Modern network architectures:**\n",
    "\n",
    "- Inception\n",
    "- ResNet\n",
    "- ResNeXt\n",
    "- DenseNet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet\n",
    "\n",
    "AlexNet is the name of CNN, designed by  [Alex Krizhevsky](https://www.cs.toronto.edu/~kriz/) . AlexNet is considered one of the most influential papers published in computer vision, as of 2020, the paper has been cited over 70,000 times according to Google Scholar.\n",
    "\n",
    "AlexNet competed in the [ImageNet](https://en.wikipedia.org/wiki/ImageNet) Large Scale Visual Recognition Challenge (1000 different classes) on September 30, 2012. The network achieved a top-5 error of 15.3%, more than 10.8 percentage points lower than that of the runner up.\n",
    "\n",
    "AlexNet consists of 5 Convolutional Layers and 3 Fully Connected Layers.\n",
    "\n",
    "\n",
    "<img src=\"https://i.ibb.co/CBp6sfy/Alex-Net-architecture.png\" alt=\"Alex-Net-architecture\" border=\"0\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (227, 227, 3)\n",
    "n_classes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 217, 217, 96)      34944     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 217, 217, 96)      384       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 217, 217, 96)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 108, 108, 96)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 104, 104, 256)     614656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 104, 104, 256)     1024      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 104, 104, 256)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 52, 52, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 50, 50, 384)       885120    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 50, 50, 384)       1536      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 50, 50, 384)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 25, 25, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 23, 23, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 23, 23, 384)       1536      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 23, 23, 384)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 21, 21, 256)       884992    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 21, 21, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 21, 21, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              104861696 \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 129,492,712\n",
      "Trainable params: 129,489,960\n",
      "Non-trainable params: 2,752\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import numpy as np\n",
    "\n",
    "alexnet = tf.keras.Sequential()\n",
    "\n",
    "# Layer 1\n",
    "alexnet.add(Conv2D(filters=96, kernel_size=(11, 11), input_shape=img_shape))\n",
    "alexnet.add(BatchNormalization())\n",
    "alexnet.add(Activation('relu'))\n",
    "alexnet.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 2\n",
    "alexnet.add(Conv2D(filters=256, kernel_size=(5, 5)))\n",
    "alexnet.add(BatchNormalization())\n",
    "alexnet.add(Activation('relu'))\n",
    "alexnet.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 3\n",
    "alexnet.add(Conv2D(filters=384, kernel_size=(3, 3)))\n",
    "alexnet.add(BatchNormalization())\n",
    "alexnet.add(Activation('relu'))\n",
    "alexnet.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 4\n",
    "alexnet.add(Conv2D(filters=384, kernel_size=(3, 3)))\n",
    "alexnet.add(BatchNormalization())\n",
    "alexnet.add(Activation('relu'))\n",
    "\n",
    "# Layer 5\n",
    "alexnet.add(Conv2D(filters=256, kernel_size=(3, 3)))\n",
    "alexnet.add(BatchNormalization())\n",
    "alexnet.add(Activation('relu'))\n",
    "alexnet.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "## Fully-connected\n",
    "# Layer 6\n",
    "alexnet.add(Flatten())\n",
    "alexnet.add(Dense(4096, activation='relu'))\n",
    "alexnet.add(Dropout(0.5))\n",
    "\n",
    "# Layer 7\n",
    "alexnet.add(Dense(4096, activation='relu'))\n",
    "alexnet.add(Dropout(0.5))\n",
    "\n",
    "# Layer 8\n",
    "alexnet.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "alexnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(alexnet, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_loss_accuracy_evolution(history):\n",
    "\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Sparse Categorical Crossentropy')\n",
    "    ax1.plot(hist['epoch'], hist['loss'], label='Train Error')\n",
    "    ax1.plot(hist['epoch'], hist['val_loss'], label='Val Error')\n",
    "    ax1.grid()\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.plot(hist['epoch'], hist['accuracy'], label='Train Accuracy')\n",
    "    ax2.plot(hist['epoch'], hist['val_accuracy'], label='Val Accuracy')\n",
    "    ax2.grid()\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet-5\n",
    "\n",
    "LeNet-5 is a classical CNN that was introduced back to 1998, is designed to recognize the digits from 0 to 9. \n",
    "\n",
    "LeNet-5 is such a classical model that it consists of two convolution layers followed by average pooling layers for each and apply three fully connected layers in the end of the network. In the original paper **sigmoid** activation function was used.\n",
    "\n",
    "<img src=\"https://i.ibb.co/y4KJH39/LeNet-5.png\" alt=\"LeNet-5\" border=\"0\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "img_shape = (28, 28, 1)\n",
    "\n",
    "# load data, split between train and test sets\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = train_images.astype(\"float32\") / 255\n",
    "x_test = test_images.astype(\"float32\") / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "plt.imshow(train_images[4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "\n",
    "Model original architecture:\n",
    "\n",
    "- **First conv layer**: 6 filters of 3x3, sigmoid activation + Average Pooling\n",
    "- **Second conv layer**: 16 filters of 3x3, sigmoid activation + Average Pooling\n",
    "- **Flatten layer**\n",
    "- **Dense layer**: 120 units, sigmoid activation\n",
    "- **Dense layer**: 84 units, sigmoid activation\n",
    "- **Output layer**: 10 units (classes), softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The added layer must be an instance of class Layer. Found: Ellipsis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-043246c63c7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel_old\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel_old\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m model_old.compile(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m       raise TypeError('The added layer must be '\n\u001b[0m\u001b[1;32m    183\u001b[0m                       \u001b[0;34m'an instance of class Layer. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                       'Found: ' + str(layer))\n",
      "\u001b[0;31mTypeError\u001b[0m: The added layer must be an instance of class Layer. Found: Ellipsis"
     ]
    }
   ],
   "source": [
    "## sigmoids and layers.AveragePooling2D\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model_old = keras.Sequential()\n",
    "\n",
    "model_old.add(layers.Conv2D(filters=6, kernel_size=(3, 3), activation='sigmoid', input_shape=img_shape))\n",
    "model_old.add(...)\n",
    "\n",
    "model_old.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "history = model_old.fit(x_train, y_train, batch_size=32 ,epochs=5, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_old.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "show_loss_accuracy_evolution(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_errors(x_test, model, y_test, n_samples=10):\n",
    "    labels = y_test.argmax(axis=-1).astype(np.int32)\n",
    "    predictions = model.predict(x_test)\n",
    "    preds = predictions.argmax(axis=-1).astype(np.int32)\n",
    "    preds_prob = predictions.max(axis=-1)\n",
    "\n",
    "    bad_pred_inds = np.where(preds != labels)[0]\n",
    "    n_samples = min(len(bad_pred_inds), n_samples)\n",
    "    samples_inds = np.random.choice(bad_pred_inds, n_samples)\n",
    "    for ind in samples_inds:\n",
    "        title = 'Predicted : {0}, real : {1}, prob: {2:.2f}'.format(\n",
    "            int(preds[ind]), labels[ind], preds_prob[ind])\n",
    "        plt.imshow(x_test[ind, :, :, 0])\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "        print()\n",
    "    return\n",
    "\n",
    "\n",
    "show_errors(x_test, model_old, y_test, n_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve the model\n",
    "\n",
    "- **First conv layer**: 6 filters of 3x3, relu activation + Max Pooling\n",
    "- **Second conv layer**: 16 filters of 3x3, relu activation + Max Pooling\n",
    "- **Flatten layer**\n",
    "- **Dense layer**: 120 units, relu activation\n",
    "- **Dense layer**: 84 units, relu activation\n",
    "- **Output layer**: 10 units (classes), softmax activation\n",
    "Use relu instead sigmoids and MaxPooling2D, try any regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## relu and layers.MaxPooling2D\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(...)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "history = model.fit(x_train, y_train, batch_size=128 ,epochs=5, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(results[0]))\n",
    "print('Test Accuracy: {}'.format(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "show_loss_accuracy_evolution(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_errors(x_test, model, y_test, n_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a photo and predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_image(image_path, target_size=None, grayscale=False):\n",
    "    image = tf.keras.preprocessing.image.load_img(image_path,\n",
    "                                                  target_size=target_size,\n",
    "                                                  grayscale=grayscale)\n",
    "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    image = image.astype(np.uint8)\n",
    "    return image\n",
    "\n",
    "\n",
    "#img_path = '../images/digit.png'\n",
    "url = 'https://i.ibb.co/3h7mLN1/digit.png'\n",
    "image_path = tf.keras.utils.get_file(\"digit_8.png\", url)\n",
    "img = read_image(image_path, target_size=(28, 28), grayscale=True)\n",
    "\n",
    "plt.imshow(np.squeez(img))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.expand_dims(img, 0)\n",
    "x = x / 255.0\n",
    "print(x.shape)\n",
    "print('Old model character: {0} with probability: {1}'.format(model_old.predict(x).argmax(), model_old.predict(x).max()))\n",
    "print('Improved model character: {0} with probability: {1}'.format(model.predict(x).argmax(), model.predict(x).max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG-16\n",
    "\n",
    "VGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper “Very Deep Convolutional Networks for Large-Scale Image Recognition”. The model achieves 92.7% top-5 test accuracy in ImageNet. VGG16 was trained for weeks and was using NVIDIA Titan Black GPU’s\n",
    "\n",
    "<img src=\"https://i.ibb.co/QYfBNKm/vgg-16.png\" alt=\"vgg-16\" border=\"0\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_VGG16 = tf.keras.applications.VGG16(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    classes=1000,\n",
    "    input_shape=(256, 256, 3),\n",
    ")\n",
    "model_VGG16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize filter shapes\n",
    "for i, layer in enumerate(model_VGG16.layers):\n",
    "    # check for convolutional layer\n",
    "    if 'conv' not in layer.name:\n",
    "        continue\n",
    "    # get filter weights\n",
    "    filters, biases = layer.get_weights()\n",
    "    print(i, layer.name, filters.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_folder = '../../images'\n",
    "url = 'https://i.ibb.co/q5TFmqh/bird.jpg'\n",
    "#url = 'https://i.ibb.co/vd1SqSM/The-grandeur-of-the-Taj-Mahal-and-its-intricate-stone-inlays-immediately-greets-the-visitor-upon-ent.jpg'\n",
    "image_path = tf.keras.utils.get_file(\"bird.jpg\", url)\n",
    "# image_path = '../../images/bird.jpeg'\n",
    "image = read_image(image_path, target_size=(256, 256))\n",
    "img = np.expand_dims(image, axis=0)\n",
    "plt.imshow(image, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "outputs_dict = {layer.name: layer.output for layer in model_VGG16.layers}\n",
    "# Set up a model that returns the activation values for every layer in\n",
    "# VGG19 (as a dict).\n",
    "feature_extractor = keras.Model(inputs=model_VGG16.inputs,\n",
    "                                outputs=outputs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the features of the image\n",
    "features = feature_extractor(img)\n",
    "print(features.keys())\n",
    "#print(list(features.items())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature map for first hidden layer\n",
    "feature_maps = features['block1_conv1']\n",
    "print('feature_maps first hidden layer shape: ', feature_maps.shape)\n",
    "# plot all 64 maps in an 8x8 squares\n",
    "square = 8\n",
    "ix = 1\n",
    "plt.figure(figsize=(30, 30))\n",
    "for _ in range(square):\n",
    "    for _ in range(square):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(square, square, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in viridis or gray\n",
    "        plt.imshow(feature_maps[0, :, :, ix - 1], cmap='viridis')\n",
    "        ix += 1\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block3_conv3\n",
    "feature_maps = features['block3_conv3']\n",
    "print('feature_maps shape: ', feature_maps.shape)\n",
    "\n",
    "# plot all 256 maps in an 16x16 squares\n",
    "square = 16\n",
    "ix = 1\n",
    "plt.figure(figsize=(40, 40))\n",
    "for _ in range(square):\n",
    "    for _ in range(square):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(square, square, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        plt.imshow(feature_maps[0, :, :, ix - 1], cmap='viridis')\n",
    "        ix += 1\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = features['block1_conv2'] \n",
    "print('feature_maps shape: ', feature_maps.shape)\n",
    "for i in range(feature_maps.shape[-1]):\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(feature_maps[0, :, :, i], cmap='viridis')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = features['block1_pool'] \n",
    "print('feature_maps shape: ', feature_maps.shape)\n",
    "for i in range(feature_maps.shape[-1]):\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(feature_maps[0, :, :, i], cmap='viridis')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
